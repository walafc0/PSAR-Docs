\chapter{Architecture des ordinateurs}

  \lettrine[nindent=0em,lines=3]{N} ous allons dans un premier temps revenir sur
  le fonctionnement des ordinateurs indépendement de l'architecture
  considérée. Ainsi, les différents concepts seront expliqués dans leur
  généralités avant d'exposer les spécificités de l'architecture NUMA. Cette
  partie nous permettra de faire un bilan général des connaissances acquises
  lors des modules d'architecture et de noyau lors de cette année.


  \section{Principes généraux}

    \subsection{Le processeur}

    Le processeur est le coeur de l'ordinateur.  Son rôle est d'effectuer tous
    les calculs, lectures et écritures sur les données. Un processeur est
    cadencé à une certaine vitesse qui se mesure en cycle et qui varie selon les
    modèles. À chaque cycle, le processeur est chargé d'aller chercher une
    instruction dans la mémoire, la décoder et l'exécuter.

    \subsubsection{Les caches}

      L'accès à la mémoire est une opération très couteuse, entre 100 et 2000
      cycles\cite{Lepers2014}. Afin d'accélérer ce processus, les ingénieurs ont
      créé les mémoires cache, ou plus communément caches. Les caches sont une
      zone mémoire de taille infiniment inférieure que la mémoire principale
      (entre 4Ko et 12Mo) mais en contrepartie beaucoup plus rapide d'accès (de
      3 à 5 cycles pour les caches de premier niveau) du fait de leur
      composition matérielle. Il peut y avoir plusieurs niveaux de cache, comme
      on le voit sur la figure ~\ref{f:cacheorga}.

      \myfig{0.5}{img/cacheorga.png}{Schéma de l'organisation des caches}{f:cacheorga}

      Chaque niveau de cache contient une partie des données de la mémoire
      RAM. La construction des caches se base sur deux grandes notions en
      architecture: la \textbf{localité spatiale} et la \textbf{localité
        temporelle}. La localité spatiale est une loi disant que si une donnée
      est accédée à une adresse A, il existe une très grande probabilité que la
      prochaine donnée à vouloir être accédée par le proceseur soit à l'adresse
      A+1. Cette loi se base sur le principe de la \textbf{séquentialité du
        stockage des données}. La localité temporelle nous dit quant à elle que
      si une donnée est accédée à un instant T, elle sera très probablement
      accédée de nouveau dans un temps futur très proche. Afin d'accélérer
      encore plus le traitement des caches de premiers niveaux, le cache L1 est
      coupé en deux parties: une contenant les identifiants d'instructions (les
      branchement, chargements\ldots) à charger, et l'autre les données
      relatives à ces instructions.

      Prenons une configuration simple avec un seul niveau de cache. Lorsque le
      processeur demande d'accéder à une donnée, il va maintenant le demander au
      cache et non directemment à la mémoire princiaple. Si le cache possède
      cette information il va déclencher un signal de \textbf{Cache Hit} et
      répondre au processeur. Dans le cas inverse, il va déclencher un signal de
      \textbf{Cache Miss}. Il va ensuite devoir accéder à la mémoire pour
      obtenir l'information demandée par le processeur et, en utilisant les lois
      de localité spatiale et temporelle évoquées précédemment, il va rapatrier
      les $X$ données suivant celle initialement demandée. Ainsi, pour les $X$
      prochains accès, il est très probable que le processeur obtiendra
      directement du cache ce qu'il veut, et les performances auront été
      significativement améliorées.

      Ce fonctionnement est le même si l'on ajoute plusieurs niveaux de cache,
      comme sur la figure ~\ref{f:caches}

      \myfig{0.37}{img/caches.png}{Schéma des niveaux caches d'un processeur}{f:caches}

      Le cache L3 est le plus gros, et il est généralement partagé en les
      processeurs. Les cache L1 est toujours local à un coeur. Les caches L2
      sont généralement locaux ou alors peuvent être partagés entre coeurs. Les
      caches de niveaux L3 sont eux généralement partagés entre les processeurs.

    \subsection{La mémoire}

      Nous avons précédement évoqué les accès mémoire sans rentrer dans les
      détails. En effet, la gestion des adresses est un processus assez
      complexe. Lorsque l'on parle de mémoire, il est important de différencier
      la mémoire physique de la mémoire virtuelle. La mémoire physique est,
      comme son nom l'indique, physiquement présente dans la machine. C'est que
      qu'on appelle les \og barettes de RAM\fg. La mémoire virtuelle est une
      couche d'abstraction de la mémoire physique, et c'est cette abstraction
      que le système d'exploitation voit et manipule. Ainsi, à chaque
      instruction nécessitant un accès, le processeur est obligé d'effectuer une
      traduction d'adresse. Cette opération est effectuée par l'\textbf{Unité de
        Gestion Mémoire}, ou \textbf{MMU} (Memory Management Unit). La MMU fait
      partie intégrante du CPU, bien que sur certains modèles elle soit sur un
      circuit séparé.\newline

      \myfig{0.5}{img/mmu2.png}{Schéma des communications entre le processeur et
        la MMU}{f:mmu2}

      La mémoire physique est découpé en partie logique appelées pages ou
      segments, selon le type de découpage. De nos jours les mémoires sont à la
      fois paginée et segmentée pour des raisons de performances. Un segment est
      un espace mémoire de taille variable, alors qu'une page a une taille fixe
      (généralement 4Ko). Toutes ces données sont stockées et gérées par le
      système d'exploitation dans la \textbf{table des pages}. Sur les machines
      x86-64, la table des pages est composée de quatre niveaux d'indirections
      (figure ~\ref{f:tdp}). Chaque entrée dans les trois premiers niveaux
      redirige vers le niveau suivant. L'entrée dans le dernier niveau est
      l'adresse de l'espace dans la mémoire physique. Afin de réaliser cette
      traduction, les adresses physiques sont coupées en cinq parties
      distinctes. Les quatres premières indiquent les indexes dans les niveaux
      d'indirection. La dernière partie contient l'offset de la donnée dans
      l'espace physique.
    
      \myfig{0.3}{img/tdp.png}{Découpage d'une adresse virtuelle en niveaux
      d'indirection}{f:tdp}

      Un des composant essentiel à la MMU est le \textbf{Translation lookaside
        buffer} (TLB). Ce composant agit comme un cache pour les accès
      mémoire. Il mémorise les derniers accès correspondant aux dernières pages
      auxquelles le processeur a dû accéder, permettant d'améliorer grandement
      les temps d'accès à la mémoire. La MMU, via le TLB connait donc toutes les
      équivalences entre adresse virtuelles et physiques.\newline

      Voici une illustration représentant les échanges entre les processeurs,
      les différents niveaux de caches, la MMU et la mémoire RAM. Ici il n'y a
      que deux niveaux de caches et le deuxième est partagé entre les
      processeurs.
    
      \myfig{0.65}{img/cache_mmu_global2.jpg}{Schéma des interactions concernant
        les adresses et les données entre les différents composant d'un
        ordinateur}{f:memory}


    \subsection{Limites}
  
      Le type d'architecture présenté ici est utilisé partout dans nos machines
      à l'heure actuelle. Le besoin de recourir à d'autres solutions se cantonne
      uniquement à des utilisation particulière des ordinateurs. Prenons un
      serveur réalisant une quantité de calcul énorme à la minute. Comment
      réagira le contrôleur mémoire à toutes ces demandes d'accès simultannées ?
      Si ce même serveur à maintenant besoin d'effectuer un grand nombre
      d'entrées/sorties sur le disque dur, comment pouvoir garantir l'efficacité
      de cette opération sans nuire à celle des autres ?  C'est dans cette
      optique qu'on été pensée les architectures NUMA: pouvoir répondre à ces
      problématiques particulières qui ne correspondent pas à celle du grand
      public.

\newpage

  \section{L'architecture NUMA}
    
    \textit{Dans cette partie nous détaillerons uniquement l'architecture de la
      machine que nous avons eu à notre disposition durant ce projet. Les
      concepts sont les mêmes sur d'autres machines NUMA, seuls les
      caractéristiques technique ou les méthodes d'implémentation diffèrent. La
      machine en question est le modèle \og Magny Cour\fg et est produit par le
      constructeur américain
      AMD.\footnote{http://products.amd.com/pages/optoeroncpudetail.aspx?id=644}\newline}


    L'architecture NUMA a été pensée dans le but d'accélérer la vitesse de
    traitement de l'information par les machines ayant de gros besoin en
    ressources. Cette amélioration est rendue possible grâce à différents
    concepts que nous allons détailler dans par la suite.

    \subsection{Les noeuds}

      L'intérêt et la puissance de cette architecture repose sur la notion de
      \og noeud\fg. Un noeud est un ensemble de coeurs de processeurs regroupés
      sur une même puce. Chaque noeud contient un certain nombre de coeurs d'un
      processeur sur une puce, et celles-ci sont assemblées par deux sur un
      composant appelé \textbf{Multichip Module}, ou MCM. Notre machine dipose
      de quatre processeurs AMD Opteron 6172 de douze coeurs chacun. On a donc
      quatre MCM, contenant douze coeurs chacun, répartis en deux noeuds de six
      coeurs.\newline

      Les différents noeuds sont interconnectés entre eux via des bus appelés
      \textbf{liens Hypertransport} (HT links) et possèdent, selon les machines,
      un certain nombre de contrôleurs mémoire et d'entrées/sorties. Nous
      détaillerons par la suite les différentes caractéristique de ces
      périphériques. Sur notre machine, chaque noeuds est composé de quatre
      liens Hypertransport, deux contrôleurs d'accès mémoire, et certain ont un
      contrôleur d'entrées/sorties,

      \myfig{0.4}{img/mcm.png}{Illustration d'un MultiChip module}{f:mcm}

    \subsection{La communication entre les noeuds}

      La techonologie des liens Hypertransport permet l'interconnexion entre
      processeurs. Développée depuis 2001, elle est principalement utilisée par
      AMD dans ses machines NUMA mais également dans les systèmes
      MIPS\footnote{Microprocessor without Interlocked Pipeline Stages} les plus
      récents. Dans notre machine, il existe deux type de liens HT, la
      différence étant leur capacité de transport:

      \begin{itemize}
        \item[HT3 x8:] bande passante maximale de 6.4GB/s. 2.8GB/s obervés sur
          notre machine\cite{Lepers2014}
        \item[HT3 x16:] bande passante maximale de 25.6GB/s.De 3GB/s à 6.6GB/s
          sur notre machine\cite{Lepers2014}
      \end{itemize}

      Au sein d'un MCM, les noeuds sont reliés entre eux via un lien HT3 x8 et
      un lien HT3 x16. Les noeuds entre MCM sont connectés entre eux de la même
      manière. Néanmoins, tous les noeuds ne sont pas reliés entre eux, créant
      ainsi une \textbf{toologie} particulière pour la machine considérée. Comme
      le montre l'illustration ~\ref{f:topo} issue de la documentation
      officielle du Magny Cour, on voit que les couples de noeuds \{P1,P6\} et
      \{P2-P5\} ne sont pas reliés sur notre machine.

      \myfig{0.6}{img/topo.png}{Schéma de la topologie du \textit{Magny
          Cour}}{f:topo}

      Les communciations sur les liens HT se font par
      paquets\cite{CacheHierarchy}. Ils sont uniformes et comprennent un header
      contenant l'émetteur et le récepteur de la requête. Pour chaque couple de
      MCM, un noeud est défini comme étant le routeur. Si un noeud a besoin
      d'accéder à une donnée dont il connait le propriétaire mais n'est pas
      directement connecté à lui, alors il passera par le noeud routeur afin de
      satisfaire sa demande.

    \subsection{Les contrôleurs d'entrées/sorties}

      L'illustration ~\ref{f:topo} nous montre également une des particularité
      du Magny Cour: la disposition des contrôleurs d'entrée/sortie. Ces
      derniers ont été placés uniquement sur les noeuds P1 et P5. Cette
      disposition permet une diminutation de la latence des échanges entre les
      noeuds mais peut être source de problème si les applications solicitant la
      machine doivent faire beaucoup d'entrées/sorties. C'est notamment pour
      cela qu'il est important de savoir quelles applications seront utilisées
      sur une machine avant de procéder à l'achat. Dans la documentation du
      Magny Cour, il est dit que cette machine a été étudiée pour du calcul
      haute performance et non pour des entrées/sorties.

    \subsection{Les accès mémoire}

      Dans notre configuration, chaque noeud possède deux contrôleurs mémoire,
      et donc une partie de la mémoire physique. Le découpage est fait
      équitablement de la façon suivante: le noeud numéro 1 possède la partie
      basse de la mémoire, le noeud numéro 2 la partie suivante, et ainsi de
      suite jusqu'a noeud numéro 8.

      L'accès par un noeud à une donnée présente dans sa partie de la mémoire
      est appelé \textbf{accès local}. Dans le cas contraire, c'est un
      \textbf{accès distant}. Les accès distants sont possibles grâce au HT
      Links. Ces derniers sont chargés d'assurer la communication entre les
      noeuds, que ce soit pour les demandes d'accès aux données que pour la
      cohérence des caches. Le coup d'un accès distant est environ deux fois
      plus cher en terme de cycles (350 au lieu de 100).

    \subsection{Les caches}

      Comme toute machine actuelle, le Magny Cour possède différents niveaux de
      cache: L1, L2 et L3. Les caches L1 et L2 sont locaux aux CPU, tandis que
      les caches L3 sont partagés. Leur taille est respectivement de 64Kb, 512Kb
      et 6Mb. Le cache L2 est partagé entre les coeurs d'un MCM.

      \subsubsection{La cohérence des caches}

        Une des parties les plus complexes de la machine au niveau matériel est
        le maintient d'une cohérence entre les caches. Cette opération est régie
        par le \textbf{protocole MOESI}, acronyme de Modified, Owned, Exclusive,
        Shared, Invalid. Ces noms caractérisent les différents états possibles
        pour une ligne de cache. A un instant donné, une ligne est dans un et un
        seul état\cite{AMD_dev_2}. Il est nécessaire de maintenir une telle
        cohérence pour deux raisons:

        \begin{itemize}
          \item lors de la migration d’une tâche d’un processeur à un
            autre. Sans synchronisation, la tâche pourrait poursuivre son
            exécution sur le nouveau processeur en utilisant des données
            anciennes qu’elle a pourtant modifiées depuis.
          \item un cache sait quand une de ses lignes a été copiée sur un autre
            cache. Lorsqu'un processus fait une demain d'accès distant à une
            donnée, si le premier cache à recevoir cette demande possède la
            donnée mais dans l'état \textbf{Invalid}, il ne répondra pas.
        \end{itemize}

        Ce système est très efficace en terme de cohérence, mais il peut parfois
        être lourd en terme de consommation de bande passante. Une expérience
        faite l'année précédente sur une machine avec 4 noeuds et 16 coeurs a
        relevé un maximum de consommation en bande passante de
        15\%\cite{Lepers2014}

  \section{Le système d'exploitation}

    Dans le cadre de ce projet, nous avons avons accumuler de nouvelles
    connaissances sur l'utilisation d'un système GNU/Linux. Nous avons ainsi
    découvert la notion de \textbf{module noyau}, que nous allons expliciter
    ici.

    \subsection{Module noyau}

      Un module est une partie du noyau que l'on peut facilement ajouter et/ou
      retirer pendant l'exécution de la machine sans pour autant devoir la
      redémarrer. C'est une avancée énorme sur le noyau Linux et BSD et cela
      implique un changement profond dans la définition du noyau: il n'est plus
      \og monolithique\fg mais microlithique. {\color{red}TODO: c'est vraiment
        utile ça ?  Parce que va falloir expliquer ce qu'est un kernel
        mono/micro-lithique.} Les modules peuvent être de types très variés. Il
      peuvent assurer le fonctionnement des périphériques wifi, usb,
      l'utilisation de machine virtuelles\ldots  \newline

      L'ajout de module se fait via les commandes \textbf{insmod} et
      \textbf{rmmod}. Il existe également un outil plus évolué appelé
      \textbf{modprobe} qui réalise ces deux opérations. Ce dernier étant
      beaucoup trop haut niveau pour notre projet, nous ne l'avons pas
      utilisé.\newline

      Cette fonctionnalité nous permet de développer notre application non plus
      directement dans le noyau mais en utilisant un module, que nous
      chargons/déchargons à notre guise. Ainsi, il n'est pas nécessaire de
      recompiler un nouveau noyau et de le lancer sur une machine virtuelle à
      chaque test. {\color{red}TODO: vérifier si c'est pas incohérent avec la
        partie de Kevin.} En revanche, le fait de travailler directement sur le
      noyau du Magny Cour implique que si nous le \og plantons\fg, il faudra
      aller redémarrer la machine dans la salle où elle se situe, au lieu de
      simplement redémarrer une machine virtuelle.

    \subsection{Le placement mémoire}

      Sur une architecture NUMA, les performances des applications dépendent
      fortement du placement mémoire. Ce placement est contrôlé par le système
      d'exploitation via le mécanisme de pagination offert par le matériel. Le
      système d'exploitation maintient une table des pages par processus, donc
      partagée par les threads du processus. Le système s'occupe de la remplir
      et de la modifier en fonction des besoins du processus. Techniquement,
      lorsqu'un processus alloue un espace mémoire à bas niveau, le système lui
      donne une autorisation d'accès à une plage d'adresse virtuelle appelé mais
      n'y associe pas encore de page physique. Elles sont associées aux pages
      virtuelles de façon paresseuse : à chaque fois que le processus accède à
      une page virtuelle qui n'est pas encore présente dans la table des pages,
      le processeur déclenche une faute des pages. Cette faute est rattrapée par
      une fonction du système qui s'occupe alors de trouver une page physique
      libre pour l'associer à la page virtuelle ayant déclanché la
      faute.\newline
      
      Lorsque le système d'exploitation associe une page virtuelle à une page
      physique dans une table des pages, il choisit donc sur quel noeud les
      accès à la page virtuelle seront effectués en fonction de l'adresse de la
      page physique. Le système peut aussi choisir de migrer une page virtuelle
      d'un noeud vers un autre en copiant la page physique vers un autre noeud
      et en mettant à jour la table des pages.\newline

      Pour effectuer un placement mémoire optimisant les performances des
      processus, le système d'exploitation doit donc être capable de
      sélectionner sur quel noeud placer les pages virtuelles. Ce choix dépend
      directement du coeur sur lequel s'exécute le thread qui accède à la page,
      le but étant d'essayer de placer le thread et les pages auxquelles il
      accède sur le même noeud. La question devient donc, quels sont les threads
      qui demandent le plus d’accès mémoire et sur quels noeuds sont-ils placés?
      Si les accès sont distants, les noeuds sont-ils très éloignés ?
      Engendrent-ils une congestion dans les interconnexions ? Si oui, quel est
      le coût d'un déplacement de page mémoire ? Toutes ces questions et tous
      ces paramètres sont à prendre en compte lors des (dé)placements en mémoire
      des données.

      \subsubsection{Stratégue de placement des données}
      
        Il existe deux façons de répartir les données dans la mémoire. La
        première est appelé \og First-Touch\fg et la seconde \og
        Interleaving\fg. La technique du First-Touch est très simple: dès qu'un
        signal Cache Miss est généré, la donnée demandée est placée dans le
        cache ayant généré ce signal. Cette méthode a l'avatange d'être très
        simple à mettre en oeuvre, mais elle entraine certain défauts que nous
        allons voir par la suite. La méthode d'Interleaving consiste quant à
        elle à séparer équitablement les données entre \og tous\fg les
        noeuds. Cette technique est moins simple a implémenter mais elle a de
        gros avantages selon l'utilisation de la machine. Ces deux stratégies ne
        sont pas liées au matériel de la machine mais bien au système
        d'exploitation et aux utilisateurs: ce sont eux qui décident quand et
        quelle stratégie appliquer.\newline

        Prenons l'exemple d'une application générant énormément d'accès
        mémoire. Le nombre de cache miss peut être très élevé si les données
        sont réparties équitablement sur différents noeuds, ou au contraire
        placées sur un seul noeud distant, avec un bus d'échange plus ou moins
        rapide avec le noeud demandant les données. Et là encore, tout dépend de
        l'application utilisant ces données. Certaines applications
        fonctionneront mieux si les données sont réparties équitablement sur les
        noeuds, et au contraire certaine verront leur performances largement
        dégradées.\newline

        Une expérience illustrant ce phénomène a été réalisée l'année
        dernière\cite{Holistic2013}. L'équipe de recherche a \og stressé\fg une
        machine NUMA avec différentes applications plus où moins gourmande en
        ressource. La mémoire est soit gérée en \og First-Touch (F)\fg, soit en
        \og interleaving (I)\fg, et dans ce cas les données sont réparties
        équitablement entre les noeuds. Voici les résultats qu'ils ont obtenus:

        \myfig{0.8}{img/numa_memory_policy}{Différences de performances en
          fonction de la politique de placement en mémoire sur une architecture
          NUMA (\textit{First-Touch ou Interleaving})}{f:memory_policy}
      
        On voit que les résultats sont largement différents selon les
        applications et la politique de gestion. Ainsi, une de nos approches
        sera d'observer le fonctionnement de la mémoire selon ces différentes
        applications, étudier les défaut de pages, comprendre pourquoi certaines
        fonctionnent mieux que d'autres, et essayer d'identifier des \og
        familles d'applications\fg ayant comme trait commun leur comportement
        vis-à-vis de la mémoire.    
    


  
