\chapter{Architecture des ordinateurs}

  \lettrine[nindent=0em,lines=3]{N} ous allons dans un premier temps revenir sur le fonctionnement des
  ordinateurs indépendement de l'architecture considérée. Ainsi, les différents
  concepts seront expliqués dans leur généralités avant d'exposer les
  spécificités de l'architecture NUMA. Cette partie nous permettra de faire un
  bilan général des connaissances acquises lors des modules d'architecture et de
  noyau lors de cette année.


  \section{Principes généraux}

    \subsection{Le processeur}

    Le processeur est le coeur de l'ordinateur.  Son rôle est d'effectuer tous
    les calculs, lectures et écritures sur les données. Un processeur est
    cadencé à une certaine vitesse qui se mesure en cycle et qui varie selon les
    modèles.. À chaque cycle, le processeur est chargé d'aller chercher une
    instruction dans la mémoire, la décoder et l'exécuter.

    \subsubsection{Les caches}

      L'accès à la mémoire est une opération très couteuse, entre 100 et 2000
      cycles\cite{Lepers2014}. Afin d'accélérer ce processus, les ingénieurs ont
      créé les mémoires cache, ou plus communément caches. Les caches sont une
      zone mémoire de taille infiniment inférieure que la mémoire principale
      (entre 4Ko et 12Mo) mais en contrepartie beaucoup plus rapide d'accès (de
      3 à 5 cycles pour les caches de premier niveau) du fait de leur
      composition matérielle. Il peut y avoir plusieurs niveaux de cache, comme
      on le voit sur la figure ~\ref{f:cacheorga}.

      \myfig{0.5}{img/cacheorga.png}{Schéma de l'organisation des caches}{f:cacheorga}

      Chaque niveau de cache contient une partie des données de la mémoire
      RAM. La construction des caches se base sur deux grandes notions en
      architecture: la \textbf{localité spatiale} et la \textbf{localité
        temporelle}. La localité spatiale est une loi disant que si une donnée
      est accédée à une adresse A, il existe une très grande probabilité que la
      prochaine donnée à vouloir être accédée par le proceseur soit à l'adresse
      A+1. Cette loi se base sur le principe de la \textbf{séquentialité du
        stockage des données}. La localité temporelle nous dit quant à elle que
      si une donnée est accédée à un instant T, elle sera très probablement
      accédée de nouveau dans un temps futur très proche. Afin d'accélérer
      encore plus le traitement des caches de premiers niveaux, le cache L1 est
      coupé en deux parties: une contenant les identifiants d'instructions (les
      branchement, chargements\ldots) à charger, et l'autre les données
      relatives à ces instructions.

      Prenons une configuration simple avec un seul niveau de cache. Lorsque le
      processeur demande d'accéder à une donnée, il va maintenant le demander au
      cache et non directemment à la mémoire princiaple. Si le cache possède
      cette information il va déclencher un signal de \textbf{Cache Hit} et
      répondre au processeur. Dans le cas inverse, il va déclencher un signal de
      \textbf{Cache Miss}. Il va ensuite devoir accéder à la mémoire pour
      obtenir l'information demandée par le processeur et, en utilisant les lois
      de localité spatiale et temporelle évoquées précédemment, il va rapatrier
      les $X$ données suivant celle initialement demandée. Ainsi, pour les $X$
      prochains accès, il est très probable que le processeur obtiendra
      directement du cache ce qu'il veut, et les performances auront été
      significativement améliorées.

      Ce fonctionnement est le même si l'on ajoute plusieurs niveaux de cache,
      comme sur la figure ~\ref{f:caches}

      \myfig{0.37}{img/caches.png}{Schéma des niveaux caches d'un processeur}{f:caches}

      Le cache L3 est le plus gros, et il est généralement partagé en les
      processeurs. Les cache L1 est toujours local à un coeur. Les caches L2
      sont généralement locaux ou alors peuvent être partagés entre coeurs. Les
      caches de niveaux L3 sont eux généralement partagés entre les processeurs.

    \subsection{La mémoire}

      Nous avons précédement évoqué les accès mémoire sans rentrer dans les
      détails. En effet, la gestion des adresses est un processus assez
      complexe. Lorsque l'on parle de mémoire, il est important de différencier
      la mémoire physique de la mémoire virtuelle. La mémoire physique est,
      comme son nom l'indique, physiquement présente dans la machine. C'est que
      qu'on appelle les \og barettes de RAM\fg. La mémoire virtuelle est une
      couche d'abstraction de la mémoire physique, et c'est cette abstraction
      que le système d'exploitation voit et manipule. Ainsi, à chaque
      instruction nécessitant un accès, le processeur est obligé d'effectuer une
      traduction d'adresse. Cette opération est effectuée par l'\textbf{Unité de
        Gestion Mémoire}, ou \textbf{MMU} (Memory Management Unit). La MMU fait
      partie intégrante du CPU, bien que sur certains modèles elle soit sur un
      circuit séparé.\newline

      \myfig{0.5}{img/mmu2.png}{Schéma des communications entre le processeur et
        la MMU}{f:mmu2}

      La mémoire physique est découpé en partie logique appelées pages ou
      segments, selon le type de découpage. De nos jours les mémoires sont à la
      fois paginée et segmentée pour des raisons de performances. Un segment est
      un espace mémoire de taille variable, alors qu'une page a une taille fixe
      (généralement 4Ko). Toutes ces données sont stockées et gérées par le
      système d'exploitation dans la \textbf{table des pages}. Sur les machines
      x86-64, la table des pages est composée de quatre niveaux d'indirections
      (figure ~\ref{f:tdp}). Chaque entrée dans les trois premiers niveaux
      redirige vers le niveau suivant. L'entrée dans le dernier niveau est
      l'adresse de l'espace dans la mémoire physique. Afin de réaliser cette
      traduction, les adresses physiques sont coupées en cinq parties
      distinctes. Les quatres premières indiquent les indexes dans les niveaux
      d'indirection. La dernière partie contient l'offset de la donnée dans
      l'espace physique.
    
      \myfig{0.3}{img/tdp.png}{Découpage d'une adresse virtuelle en niveaux
      d'indirection}{f:tdp}

      Un des composant essentiel à la MMU est le \textbf{Translation lookaside
        buffer} (TLB). Ce composant agit comme un cache pour les accès
      mémoire. Il mémorise les derniers accès correspondant aux dernières pages
      auxquelles le processeur a dû accéder, permettant d'améliorer grandement
      les temps d'accès à la mémoire. La MMU, via le TLB connait donc toutes les
      équivalences entre adresse virtuelles et physiques.


      Voici une illustration représentant les échanges entre les processeurs,
      les différents niveaux de caches et la MMU et la mémoire RAM. Ici il n'y a
      que deux niveaux de caches et le deuxième est partagé entre les
      processeurs.
    
      \myfig{0.65}{img/cache_mmu_global2.jpg}{Schéma des interactions concernant
        les adresses et les données entre les différents composant d'un
        ordinateur}{f:memory}


    \subsection{Limites}
  
      Le type d'architecture présenté ici est utilisé partout dans nos machines
      à l'heure actuelle. Le besoin de recourir à d'autres solutions se cantonne
      uniquement à des utilisation particulière des ordinateurs. Prenons un
      serveur réalisant une quantité de calcul énorme à la minute. Comment
      réagira le contrôleur mémoire à toutes ces demandes d'accès simultannées ?
      Si ce même serveur à maintenant besoin d'effectuer un grand nombre
      d'entrées/sorties sur le disque dur, comment pouvoir garantir efficacité ?
      C'est dans cette optique qu'on été pensée les architectures NUMA: pouvoir
      répondre à ces problématiques particulières qui ne correspondent pas à
      celle du grand public.

\newpage

  \section{L'architecture NUMA}
    
    \textit{Dans cette partie nous détaillerons uniquement l'architecture de la
      machine que nous avons eu à notre disposition durant ce projet. Les
      concepts sont les mêmes sur d'autres machines NUMA, seuls les
      caractéristiques technique ou les méthodes d'implémentation diffèrent. La
      machine en question est appelé \og Magny Cour\fg et est produit par le
      constructeur américain
      AMD.\footnote{http://products.amd.com/pages/optoeroncpudetail.aspx?id=644}\newline}


    L'architecture NUMA a été pensée dans le but d'accélérer la vitesse de
    traitement de l'information par les machines ayant de gros besoin en
    ressources. Cette amélioration est rendue possible grâce à différents
    concepts que nous allons détailler dans par la suite.

  \subsection{Les noeuds}

    L'intérêt et la puissance de cette architecture repose sur la notion de \og
    noeud\fg. Un noeud est un ensemble de coeurs de processeurs regroupés sur
    une même puce. Chaque noeud contient un certain nombre de coeurs d'un
    processeur sur une puce, et celles-ci sont assemblées par deux sur un
    composant appelé \textbf{Multichip Module}, ou MCM. Notre machine dipose de
    quatre processeurs AMD Opteron 6172 de douze coeurs chacun. On a donc quatre
    MCM, contenant douze coeurs chacun, répartis en deux noeuds de six
    coeurs.\newline

    Les différents noeuds sont interconnectés entre eux via des bus appelés
    \textbf{liens Hypertransport} (HT links) et possèdent, selon les machines,
    un certain nombre de contrôleurs mémoire et d'entrées/sorties. Nous
    détaillerons par la suite les différentes caractéristique de ces
    périphériques. Sur notre machine, chaque noeuds est composé de quatre liens
    Hypertransport, deux contrôleurs d'accès mémoire, et certain ont un
    contrôleur d'entrées/sorties,

    \myfig{0.4}{img/mcm.png}{Illustration d'un MultiChip module}{f:mcm}

  \subsection{La communication entre les noeuds}

    La techonologie des liens Hypertransport permet l'interconnexion entre
    processeurs. Développée depuis 2001, elle est principalement utilisée par
    AMD dans ses machines NUMA mais également dans les systèmes
    MIPS\footnote{Microprocessor without Interlocked Pipeline Stages} les plus
    récents. Dans notre machine, il existe deux type de liens HT, la différence
    étant leur capacité de transport:

    \begin{itemize}
      \item[HT3 x8:] bande passante maximale de 6.4GB/s. 2.8GB/s obervés sur notre
      machine\cite{Lepers2014}
      \item[HT3 x16:] bande passante maximale de 25.6GB/s.De 3GB/s à 6.6GB/s sur
      notre machine\cite{Lepers2014}
    \end{itemize}

    Au sein d'un MCM, les noeuds sont reliés entre eux via un lien HT3 x8 et un
    lien HT3 x16. Les noeuds entre MCM sont connectés entre eux de la même
    manière. Néanmoins, tous les noeuds ne sont pas reliés entre eux, créant
    ainsi une \textbf{toologie} particulière pour la machine considérée. Comme
    le montre l'illustration ~\ref{f:topo} issue de la documentation officielle
    du Magny Cour, on voit que les couples de noeuds \{P1,P6\} et \{P2-P5\} ne
    sont pas reliés sur notre machine.

    \myfig{0.6}{img/topo.png}{Schéma de la topologie du \textit{Magny Cour}}{f:topo}

    Les communciations sur les liens HT se font par
    paquets\cite{CacheHierarchy}. Ils sont uniformes et comprennent un header
    contenant l'émetteur et le récepteur de la requête. Pour chaque couple de
    MCM, un noeud est défini comme étant le routeur. Si un noeud a besoin
    d'accéder à une donnée dont il connait le propriétaire mais n'est pas
    directement connecté à lui, alors il passera par le noeud routeur afin de
    satisfaire sa demande.


  \subsection{Les contrôleurs d'entrées/sorties}

    L'illustration ~\ref{f:topo} nous montre également une des particularité du
    Magny Cour: la disposition des contrôleurs d'entrée/sortie. Ces derniers ont
    été placés uniquement sur les noeuds P1 et P5. Cette disposition permet une
    diminutation de la latence des échanges entre les noeuds mais peut être
    source de problème si les applications solicitant la machine doivent faire
    beaucoup d'entrées/sorties. C'est notamment pour cela qu'il est important de
    savoir quelles applications seront utilisées sur une machine avant de
    procéder à l'achat. Dans la documentation du Magny Cour, il est dit que
    cette machine a été étudiée pour du calcul haute performance et non pour des
    entrées/sorties.

  \subsection{Les accès mémoire}

    Dans notre configuration, chaque noeud possède deux contrôleurs mémoire, et
    donc une partie de la mémoire physique. Le découpage est fait équitablement
    de la façon suivante: le noeud numéro 1 possède la partie basse de la
    mémoire, le noeud numéro 2 la partie suivante, et ainsi de suite jusqu'a
    noeud numéro 8.

    L'accès par un noeud à une donnée présente dans sa partie de la mémoire est
    appelé \textbf{accès local}. Dans le cas contraire, c'est un \textbf{accès
      distant}. Les accès distants sont possibles grâce au HT Links. Ces
    derniers sont chargés d'assurer la communication entre les noeuds, que ce
    soit pour les demandes d'accès aux données que pour la cohérence des
    caches. Le coup d'un accès distant est environ 15\% plus cher qu'un accès
    local.\cite{Lepers2014}

    TOFINISH

  \subsection{Les caches}

    Comme tout processeur, notre machine possède différents niveaux de cache:
    L1, L2 et L3. Les caches L1 et L2 sont locaux aux CPU, tandis que les caches
    L3 sont partagés. Leur taille est respectivement de 64Kb, 512K et 6Mb. Afin de 

    \subsubsection{La cohérence des caches}

      
    
